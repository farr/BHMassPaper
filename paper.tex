\documentclass[preprint]{aastex}

%\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{amsmath}

\newcommand{\fixme}[1]{[FIXME: #1]}
\newcommand{\Msun}{M_\odot}
\newcommand{\Mmin}{M_{\textnormal{min}}}
\newcommand{\Mmax}{M_{\textnormal{max}}}
\newcommand{\Nbin}{N_{\textnormal{bin}}}
\newcommand{\vtheta}{\vec{\theta}}
\newcommand{\order}[1]{\mathcal{O}\left( #1 \right)}

\bibliographystyle{hapj}

\begin{document}

\title{The Mass Distribution of Stellar-Mass Black Holes}

\author{Will M. Farr \and Niharika Sravan} 

\affil{Northwestern University Center for Interdisciplinary
  Exploration and Research in Astrophysics\\2145 Sheridan Rd.,
  Evanston, IL 60208}

\email{w-farr@northwestern.edu, niharika.sravan@gmail.com}

\author{Andrew Cantrell \and Laura Kreidberg \and Charles Bailyn}

\affil{Yale University Department of Astrophysics\\
  P.O. Box 208101, New Haven, CT 06520}

\email{andrew.cantrell@yale.edu, laura.kreidberg@yale.edu,
  charles.bailyn@yale.edu}

\author{Ilya Mandel\altaffilmark{1}}

\affil{MIT Kavli Institute, Cambridge, MA 02139}

\email{ilyamandel@chgk.info}

\and

\author{Vicky Kalogera} 

\affil{Northwestern University Center for Interdisciplinary
  Exploration and Research in Astrophysics\\2145 Sheridan Rd.,
  Evanston, IL 60208}

 \email{vicky@northwestern.edu}

 \altaffiltext{1}{School of Physics and Astronomy, University of
   Birmingham, Edgbaston, Birmingham B15 2TT, UK}

\begin{abstract}
  We perform a Bayesian analysis of the mass distribution of
  stellar-mass black holes using the observed masses of 15 low-mass
  X-ray binary systems undergoing Roche lobe overflow and five
  higher-mass, wind-fed X-ray binary systems.  Using MCMC
  calculations, we model the mass distribution both
  parametrically---as a power law, exponential, gaussian, combination
  of two gaussians, or log-normal distribution---and
  non-parametrically---as histograms with varying numbers of bins.  We
  provide confidence bounds on the shape of the mass distribution in
  the context of each model and compare the models with each other by
  calculating their Bayesian evidence as supported by the
  measurements.  The mass distribution of the low-mass systems is best
  fit by a power-law, while the distribution of the complete sample is
  best fit by the exponential model.  We examine the existence of a
  ``gap'' between the most massive neutron stars and least massive
  black holes, finding that the best model (the power law) fitted to
  the low-mass systems only gives a minimum black-hole mass of 4.3
  $\Msun$ (90\% confidence), while the best model (the exponential)
  fitted to all 20 systems gives a minumum black-hole mass of 4.5 and
  5.2 $\Msun$ (90\% confidence), respectively.  We conclude that our
  sample of black hole masses provides strong evidence of a gap
  between the maximum neutron star mass and minimum black hole mass.
\end{abstract}

\maketitle

\section{Introduction}
\label{sec:intro}

(X-Ray binaries are important---supernova, evolutionary processes, GW,
etc.)

\citet{Bailyn1998} examined a sample of seven transient X-ray binaries
thought to contain a black hole, concluding in a Bayesian analysis
that the mass function was strongly peaked around seven solar masses.
They found strong evidence for a ``gap'' between the least massive
black hole and the upper limit for neutron star masses of 3 $\Msun$.

A more recent study \citep{Ozel2010}, also in a Bayesian framework,
largely confirmed these findings, examining 16 low-mass X-ray binary
systems containing black holes and finding a strongly peaked
distribution at $7.8 \pm 1.2 \, \Msun$.  \citet{Ozel2010} used two
models for the mass function: a Gaussian and a decaying exponential
with a minimum ``turn-on'' mass (motivated by the analytical model of
the black-hole mass function in \citet{Fryer2001}), finding in both
cases strong evidence of a mass gap.  They argue that this mass gap
cannot be explained by observational selection effects.

In this work, we use observations of 20 low- and higher-mass galactic
X-ray binaries that contain black holes to constrain the mass function
for stellar mass black holes in the galaxy.  We consider separately
the 15 low-mass X-ray binaries containing Roche-lobe-filling
secondaries with masses smaller than the black hole primary, and a
complete sample including these systems and five higher-mass, wind-fed
systems where the secondary is heavier than the black hole primary.

We use a Bayesian Markov-Chain Monte Carlo analysis to explore various
models for the black hole mass function for both samples, including
those of \citet{Ozel2010} and \citet{Bailyn1998}.  We include both
parameteric models, such as a Gaussian, and non-parameteric models
where the mass function is represented by histograms with various
numbers of bins.  After computing posterior distributions for the
model parameters, we use model selection techniques (including a new
technique for efficient reversible-jump MCMC \citep{Farr2010}) to
compare the evidence for the various models from both samples.

We find, like \citet{Ozel2010} and \citet{Bailyn1998}, strong evidence
for a mass gap among the best models for both systems, with the best
model for the lower-mass systems giving a minimum black hole mass of
4.3 $\Msun$ (at 90\% confidence), and the best model for the combined
sample of lower- and high-mass systems giving a minimum black hole
mass of 4.5 $\Msun$ (at 90\% confidence).  Among the models with lower
evidence, most also have a mass gap.

The theoretical model from \citet{Fryer2001}, a decaying exponential,
considered for the low-mass systems in \citet{Ozel2010}, has
relatively low evidence.  We find that the lower-mass systems are best
described by a Gaussian (the other model from \citet{Ozel2010}).  We
find that the theoretical model from \citet{Fryer2001} is, however,
the preferred model for the combined sample, while a Gaussian has much
lower evidence in this case.  A model with two separate Gaussian peaks
also has high evidence for the combined sample of systems, suggesting
\fixme{differences in the distributions of lower- and high-mass X-ray
  binary systems}.

The structure of this paper is as follows \fixme{This will change
  dramatically}.


\section{Systems}
\label{sec:systems}

The 20 X-ray binary systems on which this study is based are listed in
Table \ref{tab:sources}.  We separate the systems into 15 low-mass
systems in which the central black hole appears to be fed by
roche-lobe overflow from the secondary, and 5 higher-mass systems in
which the black hole is fed via winds (these systems all have a
secondary that appears more massive than the black hole).  The low-
and higher-mass systems undoubtedly have different evolutionary
tracks, and therefore it is reasonable that they would have different
black-hole mass distributions.  We will first analyze the 15 low-mass
systems alone (Sections \ref{sec:models} and
\ref{sec:model-selection}), and then the complete sample of 20 systems
(Section \ref{sec:higher-mass}).

In each of these systems, spectroscopic measurements of the secondary
star provide an orbital period for the system and a half-amplitude for
the secondary's velocity curve.  These measurements can be combined
into the mass function,
\begin{equation}
  \label{eq:mass-function}
  f(M) = \frac{P K^3}{2\pi G} = \frac{M \sin^3 i}{\left( 1 + q \right)^2},
\end{equation}
where $P$ is the orbital period, $K$ is the secondary's velocity
semi-amplitude, $M$ is the black hole mass, $i$ is the inclination of
the system, and $q \equiv M_2 / M$ is the mass ratio of the system.

The mass function defines a lower limit on the mass: $f(M) < M$.  To
accurately determine the mass of the black hole, the inclination $i$
and mass ratio $q$ must be measured.  Ideally, this can be
accomplished by fitting ellipsoidal light curves and study of the
rotational broadening of spectral lines from the secondary, but even
in the most studied case (see, e.g., \citet{Cantrell2010} on A0620)
this procedure is complicated.  In particular, contributions from an
accretion disk and hot spots in the disk can significantly distort the
measured inclination and mass ratios.  For some systems (e.g.\ GS 1354
\citep{Casares2009}) strong variability completely prevents
determination of the inclination from the lightcurve; in these cases
an upper limit on the inclination often comes from the observed lack
of eclipses in the lightcurve.  In general, accurately determining $q$
and $i$ requires a careful system-by-system analysis.

For the purposes of this paper, we adopt the following simplified
approach to the estimation of the black hole mass from the observed
data.  When an observable is well-constrained, we assume that the true
value is normally distributed about the measured value with a standard
deviation equal to the quoted observational error.  This is the case
for the mass function in all the systems we use, and for many systems'
mass ratios and inclinations.  When a large range is quoted in the
literature for an observable, we take the true value to be distributed
uniformly (for the mass ratio) or isotropically (for the inclination)
within the quoted range.  Table \ref{tab:sources} gives the assumed
distribution for the observables in the 20 systems we use.  We do not
attempt to deal with the systematic biases in the observational
determination of $f$, $q$, and $i$ in any realistic way; we are
currently investigating more realistic treatments of the errors
(including observational biases that can shift the peak of the true
mass distribution away from the ``best-fit'' mass in the
observations).  This careful treatment will appear in future work.

\begin{table}
  \begin{center}
    \begin{tabular}{|l|c|c|c|l|}
      \hline
      Source & $f$ ($\Msun$) & $q$ & $i$ (degrees) & References \\
      \hline \hline
      GRS 1915 & $N(9.5, 3.0)$ & $N(0.0857, 0.0284)$ & $N(70, 2)$ &
      \citet{Greiner2001} \\
      XTE J1118 & $N(6.44, 0.08)$ & $N(0.0264, 0.004)$ & $N(68, 2)$ &
      \citet{Gelino2008} \\ & & & & \citet{Harlaftis2005} \\
      XTE J1650 & $N(2.73, 0.56)$ & $U(0, 0.5)$ & $I(50, 80)$ &
      \cite{Orosz2004} \\
      GRS 1009 & $N(3.17, 0.12)$ & $N(0.137, 0.015)$ & $I(37, 80)$ &
      \cite{Filippenko1999} \\
      A0620 & $N(2.76, 0.036)$ & $N(0.06, 0.004)$ & $N(50.98, 0.87)$ &
      \citet{Cantrell2010} \\ & & & & \citet{Neilsen2008} \\
      GRO J0422 & $N(1.13, 0.09)$ & $U(0.076, 0.31)$ & $N(45, 2)$ &
      \citet{Gelino2003} \\
      Nova Mus 1991 & $N(3.01, 0.15)$ & $N(0.128, 0.04)$ & $N(54,1.5)$
      & \cite{Gelino2001} \\
      GRO J1655 & $N(2.73,0.09)$ & $N(0.3663, 0.04025)$ & $N(70.2,
      1.9)$ & \citet{Greene2001} \\
      4U 1543 & $N(0.25, 0.01)$ & $U(0.25, 0.31)$ & $N(20.7,1.5)$ & 
      \citet{Orosz2003} \\
      XTE J1550 & $N(7.73,0.4)$ & $U(0,0.04)$ & $N(74.7, 3.8)$ &
      \citet{Orosz2010} \\
      V4641 Sgr & $N(3.13,0.13)$ & $U(0.42,0.45)$ & $N(75,2)$ &
      \citet{Orosz2003} \\
      GS 2023 & $N(6.08, 0.06)$ & $U(0.056,0.063)$ & $I(66, 70)$ &
      \citet{Charles2006} \\
      & & & & \citet{Khargharia2010} \\
      GS 1354 & $N(5.73, 0.29)$ & $N(0.12,0.04)$ & $I(50, 80)$ & 
      \citet{Casares2009} \\
      Nova Oph 77 & $N(4.86,0.13)$ & $U(0, 0.053)$ & $I(60, 80)$ &
      \citet{Charles2006} \\
      GS 2000 & $N(5.01, 0.12)$ & $U(0.035, 0.053)$ & $I(43, 74)$ &
      \citet{Charles2006} \\
      \hline \hline
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% High Mass Follows
      Cyg X1 & $N(0.251, 0.007)$ & $N(2.778, 0.386)$ & $I(23, 38)$ &
      \citet{Gies2003} \\ 
      M33 X7 & $N(0.46, 0.08)$ & $N(4.47, 0.61)$ & $N(74.6, 1)$ &
      \citet{Orosz2007} \\
      NGC 300 X1 & $N(2.6, 0.3)$ & $U(1.05, 1.65)$ & $I(60, 75)$ &
      \citet{Crowther2010} \\
      LMC X1 & $N(0.148, 0.004)$ & $N(2.91, 0.49)$ & $N(36.38, 2.02)$
      & \citet{Orosz2009} \\
      IC 10 X1 & $N(7.64, 1.26)$ & $U(0.7, 1.7)$ & $I(75, 90)$ & 
      \citet{Prestwich2007} \\
       & & & & \citet{Silverman2008} \\
       \hline
    \end{tabular}
  \end{center}
  \caption{\label{tab:sources} The source parameters for the 20 low
    mass X-ray binaries used in this work.  The first 15 systems have
    low-mass secondaries that feed the black hole via Roche lobe
    overflow; the last five systems have high-mass secondaries ($q
    \gtrsim 1$) that feed the black hole via winds.  In each line, $f$
    is the mass function for
    the compact object, $q$ is the mass ratio $M_2/M$, and $i$ is the
    inclination of the system to the line of sight.  We indicate the
    distribution used for the true parameters when computing the
    probability distributions for the masses of these systems:
    $N(\mu,\sigma)$ implies a Gaussian with mean $\mu$ and standard
    deviation $\sigma$, $U(a,b)$ is a uniform distribution between $a$ and
    $b$, and $I(\alpha,\beta)$ is an isotropic distribution between the
    angles $\alpha$ and $\beta$.}
\end{table}

From these assumptions, we can generate probability distributions for
the true mass of the black hole given the observations and errors via
the Monte Carlo method: drawing samples of $f$, $q$, and $i$ from the
assumed distributions and computing the mass implied by Equation
\eqref{eq:mass-function} gives samples of $M$ from the distribution
induced by the relationship in Equation \eqref{eq:mass-function}.
Mass distributions generated in this way for the systems used in this
work are shown in Figure \ref{fig:low-masses} and Figure
\ref{fig:high-masses}.  Note that systems for which $i$ is poorly
constrained have broad ``tails'' on their mass distributions.  These
mass distributions constitute the ``observational data'' we will use
in the remainder of this paper.

\begin{figure}
  \begin{center}
    \plotone{plots/all-masses} 
  \end{center}

  \caption{\label{fig:low-masses} The mass distributions implied by
    Equation \eqref{eq:mass-function} and the assumed distributions on
    observational parameters given in Table \ref{tab:sources} for the
    lower-mass sources.  The significant asymmetry and long tails in
    many of these distributions are the result of the non-linear
    relationship (Equation \eqref{eq:mass-function}) between $M$, $f$,
    $q$, and $i$.}
\end{figure}

\begin{figure}
  \begin{center}
    \plotone{plots/high-masses}
  \end{center}
  \caption{\label{fig:high-masses} Mass distributions for the
    wind-fed, higher-mass systems computed from the distributions on
    observed data in Table \ref{tab:sources} using Equation
    \eqref{eq:mass-function}.  The asymmetry and long tails in these
    distributions are the result of the non-linear relationship
    between $M$, $f$, $q$, and $i$.}
\end{figure}

\section{Models}
\label{sec:models}

In this section we apply a statistical analysis to various models for
the underlying mass distribution from which the low-mass systems in
Table \ref{tab:sources} were drawn.  We will treat the combined low-
and higher-mass systems in Section \ref{sec:higher-mass}.  The end
result will be the probability distribution for the parameters of each
model implied by the data from Section \ref{sec:systems} in
combination with our prior assumptions about the probability
distribution for the parameters.  Bayes' rule relates these
quantities.  For a model with parameters $\vtheta$ in the presence of
data $d$, Bayes' rule states
\begin{equation}
  \label{eq:Bayes-rule}
  p(\vtheta | d) = \frac{p(d | \vtheta) p(\vtheta)}{p(d)}.
\end{equation}
Here, $p(\vtheta|d)$, called the posterior probability distribution
function, is the probability distribution for the parameters $\vtheta$
implied by the data $d$; $p(d|\vtheta)$, called the likelihood, is the
probability of observing data $d$ given that the model parameters are
$\vtheta$; $p(\vtheta)$, called the prior, reflects our estimate of the
probability of the various model parameters in the absence of any
data; and $p(d)$, called the evidence, is an overall normalizing
constant ensuring that
\begin{equation}
  \int d\theta\, p(\vtheta|d) = 1,
\end{equation}
whence
\begin{equation}
  \label{eq:evidence-def}
  p(d) = \int d\vtheta\, p(d|\vtheta) p(\vtheta).
\end{equation}

In our context, the data are the mass distributions given in Section
\ref{sec:systems}: $d = \{ p_i(M)| i = 1, 2, \ldots, 17 \}$.  We
assume that the measurements in Section \ref{sec:systems} are
independent, so the complete likelihood is given by a product of the
likelihoods for the individual measurements.  For a model with
parameters $\vtheta$ that predicts a mass distribution $p(M|\vtheta)$
for black holes, we have
\begin{equation}
  \label{eq:likelihood-def}
  p(d|\vtheta) = \prod_i \int dM\, p_i(M) p(M|\vtheta).
\end{equation}
That is, the likelihood of an observation is the average over the
individual mass distribution implied by the observation, $p_i(M)$, of
the probability for a black hole of that mass to exist according to
the model of the mass distribution, $p(M | \vtheta)$.  We approximate
the integrals as averages of $p(M|\vtheta)$ over the Monte Carlo mass
samples drawn from the distributions in Table \ref{tab:sources} (also
see Figures \ref{fig:low-masses} and \ref{fig:high-masses}):
\begin{equation}
  p(d|\vtheta) \approx \prod_i \frac{1}{N_i} \sum_{j = 1}^{N_i} p(M_{ij} | \vtheta),
\end{equation}
where $M_{ij}$ is the $j$th sample (out of a total $N_i$) from the
$i$th individual mass distribution.

Our calculation of the likelihood of each observation does not include
any attempt to account for selection effects in the observations.  We
simply assume (almost certainly incorrectly) that any black hole drawn
from the underlying mass distribution is equally likely to be
observed.  (See also \citet{Ozel2010}.)

For a mass distribution with several parameters, $p(\vtheta | d)$
lives in a multi-dimensional space.  Exploring the entirety of this
space systematically rapidly becomes prohibitive as the number of
parameters increases.  A more efficient way to explore the
distribution $p(\vtheta | d)$ is to use a Markov Chain Monte Carlo
(MCMC) method.  MCMC methods produce a chain (sequence) of parameter
samples, $\{ \vtheta_i, i = 1, \ldots \}$, such that a particular
parameter set, $\vtheta$, appears in the sequence with a frequency
equal to its probability according to $p(\vtheta|d)$.  In this way,
regions of parameter space where $p(\vtheta|d)$ is large are sampled
densely while regions where $p(\vtheta|d)$ is small are effectively
ignored.  A Markov chain has the property that the transition
probability from one element to the next, $p(\vtheta_i \to
\vtheta_{i+1})$, depends only on the value of $\vtheta_i$, not on any
previous values in the chain.

One way to produce a sequence of MCMC samples is via the following
algorithm, first proposed by \citet{Metropolis1953}:
\begin{enumerate}
  \item Begin with the current sample, $\vtheta_i$.
  \item Propose a new sample, $\vtheta_p$, by drawing randomly from a
    ``jump proposal distribution'' with probability $Q(\vtheta_i \to
    \vtheta_p)$.  Note that $Q(\vtheta_i \to \vtheta_p)$ can depend on
    the current parameters, $\vtheta_i$, and any other ``constant''
    data, but cannot examine the history of the chain beyond the most
    recent point.  This is necessary to preserve the Markovian
    property of the chain.
  \item Compute the ``acceptance'' probability,
    \begin{equation}
      \label{eq:paccept}
      p_{\textnormal{accept}} \equiv
      \frac{p(\vtheta_p|d)}{p(\vtheta_i|d)} \frac{Q(\vtheta_p \to
        \vtheta_i)}{Q(\vtheta_i \to \vtheta_p)}
    \end{equation}
  \item With probability $\min(1,p_{\textnormal{accept}})$ ``accept''
    the propsed $\vtheta_p$, setting $\vtheta_{i+1} = \vtheta_p$;
    otherwise set $\vtheta_{i+1} = \vtheta_i$.
\end{enumerate}
This algorithm is more likely to accept a proposed jump when it
increases the posterior (the first factor in Equation
\eqref{eq:paccept}) and when it is to a location in parameter space
from which it is easy to return (the second factor in Equation
\eqref{eq:paccept}); the combination of these influences in Equation
\eqref{eq:paccept} ensures that the equilibrium distribution of the
chain is $p(\vtheta|d)$.  As $i \to \infty$ the samples $\vtheta_i$
are distributed according to $p(\vtheta|d)$.  

In practice the number of samples required before the chain
appropriately samples $p(\vtheta|d)$ depends strongly on the jump
proposal distribution; proposal distributions that often propose jumps
toward or within regions of large $p(\vtheta|d)$ can be very
efficient, while poor proposal distributions can require prohibitively
large numbers of samples before convergence.  

There is no foolproof test for the convergence of a chain.  In this
work, we test the convergence of our chains by comparing the
statistics calculated from the entire chain to statistics calculated
from only the first half of the chain; when the chain has converged,
the two calculations agree.  This is a necessary, but not sufficient,
condition for convergence.

We begin the chain at an arbitrary point in parameter space; this is
equivalent to taking a finite section of an infinite chain that begins
with the chosen point.  Every point in parameter space occurs in an
infinite chain, and no section of the chain is better than any other,
so a sufficiently long, but finite, section of the infinite chain
chosen in this manner can be representative of the statistics of the
chain as a whole.  However, because consecutive samples in a chain are
correlated with each other, the beginning of our finite chain has a
``memory'' of the starting point; we discard enough points at the
beginning of the finite chain that we can be confident it does not
retain a memory of the arbitrary starting point.  The points discarded
in this way are commonly called ``burn-in'' points.

Once we have a chain of samples from $p(\vtheta|d)$---a distribution
of distributions---the distribution for any quantity of interest can
be computed.  For example, Figure \ref{fig:dists} shows the median
values, and the 10\% and 90\% quantiles for the value of the BH mass
probability distribution at various masses.  (Recall that we are only
using the 15 low-mass observations in this Section's analysis.)  Such
a figure can be generated from the MCMC output by computing the value
of the parameterized BH mass distribution at various masses for each
parameter sample output by the MCMC, and then plotting the quantiles
at each mass.

\begin{figure}
  \begin{center}
    \plottwo{plots/dist-parametric}{plots/dist-non-parametric}
  \end{center}
  \caption{\label{fig:dists} The median values of the black hole mass
    distribution, $p(M|\theta)$, at various masses implied by the
    posterior $p(\theta|d)$ for the models discussed in Sections
    \ref{sec:parametric-models} and
    \ref{sec:non-parametric-models}.  These distributions use only
    the 15 low-mass observations in Table \ref{tab:sources} (the
    complete sample is analyzed in Section \ref{sec:higher-mass}).
    Error bars span the 10\% to 90\% range.  Note that these
    ``distributions of distributions'' are not necessarily normalized,
    and need not be ``shaped'' like the underlying model
    distributions.}
\end{figure}

It is also common to look at the one-dimensional distribution for a
single parameter obtained by integrating over all other dimensions in
parameter space; this is approximated by a histogram of the MCMC
sample values for that parameter.  Such a distribution is called the
``marginalized'' distribution.

\subsection{Priors}
\label{sec:priors}

An important part of any Bayesian analysis is the priors placed on the
parameters of the model.  The choice of priors can bias the results of
the analysis through both the shape and the range of prior support in
parameter space.  The prior should reflect the ``best guess'' for the
distribution of parameters before examining any of the data.  In the
absence of any information about the distribution of parameters, it is
best to choose a prior that is broad and uniformative to avoid biasing
the posterior as much as possible.

A prior that is independent of parameters, $\vtheta$, in some region,
called ``flat,'' results in a posterior that is proportional to the
likelihood (see Equation \eqref{eq:Bayes-rule}).  A flat prior does
not change the shape of the posterior.  However, the choice of a flat
prior is parameterization-dependent: a change of parameter from
$\vtheta$ to $\vtheta' = \vec{f}(\vtheta)$ can change a flat
distribution into one with non-trivial structure.  In this work, we
choose priors that are flat when the parameters are measured in
physical units.  In particular, for the log-normal model (Section
\ref{sec:log-normal}) the natural parameters for the distribution are
the mean, $\langle \log M \rangle$, and standard deviation,
$\sigma_{\log M}$, in $\log M$, but we choose priors that are flat in
$\langle M \rangle$ and $\sigma_M$.

The range of prior support can also affect the results of a Bayesian
analysis.  Because priors are normalized, prior support over a larger
region of parameter space results in a smaller prior probability at
each point.  Such ``wide'' priors are implicitly claiming that any
particular sub-region of parameter space is less likely than it would
be under a prior of the same shape but smaller support volume.  This
difference is important in model selection (Section
\ref{sec:model-selection}): when comparing two models with the same
likelihood, one with wide priors will seem less probable than one with
narrower priors.  Of course, priors should be wide enough to encompass
all regions of parameter space that have significant likelihood.  To
make the model comparison in Section \ref{sec:model-selection} fair,
we choose prior support in parameter space so that the allowed
parameter values for each model give distributions for which nearly
all the probability lies in the range $0 \leq M \leq 40 \Msun$.

\subsection{Parametric Models for the Black Hole Mass Distribution}
\label{sec:parametric-models}

In this subsection, we discuss the various parametric models of the
underlying black hole mass distribution that we have analyzed using
the low-mass data from Section \ref{sec:systems}.  (Analysis of these
models including the higher-mass systems appears in Section
\ref{sec:higher-mass}.)  Table \ref{tab:low-mass-parametric} gives
quantiles of the marginalized parameter distributions of the
parametric models.

\begin{table}
  \begin{center}
    \begin{tabular}{|l|c|c|c|c|c|c|}
      \hline
      Model & Parameter & 5\% & 15\% & 50\% & 85\% & 95\% \\
      \hline \hline
      Power Law (Equation \eqref{eq:power-law-dist}) & $\Mmin$ & 
      1.2786 &  4.1831 &  6.1001 &  6.5011 &  6.6250 \\
      \hline
       & $\Mmax$ & 8.5578 &  8.9214 & 23.3274 & 36.0002 & 38.8113 \\
       \hline
       & $\alpha$ & -12.4191 & -10.1894 & -6.3861 &  2.8476 &  5.6954 \\
       \hline \hline
       Decaying Exponential (Equation \eqref{eq:exp-def}) & $\Mmin$ & 
       5.0185 &  5.4439 &  6.0313 &  6.3785 &  6.5316 \\
       \hline
       & $M_0$ & 0.7796 &  0.9971 & 1.5516 &  2.4635 &  3.2518 \\
       \hline \hline
       Gaussian (Equation \eqref{eq:gaussian-def}) & $\mu$ & 
       6.6349 &  6.9130 &  7.3475 & 7.7845 & 8.0798 \\
       \hline
       & $\sigma$ & 0.7478 &  0.9050  & 1.2500 &  1.7335 & 2.1134 \\
       \hline \hline
       Two Gaussian (Equation \eqref{eq:two-gaussian-def}) & $\mu_1$ & 
       5.4506 &  6.3877 &  7.1514 &  7.6728  & 7.9803 \\
       \hline
       & $\mu_2$ & 7.2355 &  7.7387 & 12.3986 & 25.2456 & 31.4216 \\
       \hline
       & $\sigma_1$ & 0.3758 &  0.7626 &  1.2104 &  1.7981 &  2.3065 \\
       \hline
       & $\sigma_2$ & 0.2048 & 0.6421 & 1.9182 &  5.2757  & 7.2625 \\
       \hline
       & $\alpha$ & 0.0983 &  0.3526 & 0.8871 &  0.9792 &  0.9936 \\
       \hline \hline
       Log Normal (Equation \eqref{eq:log-normal-def}) & $\langle M \rangle$ & 
       6.7619 &  7.0122 &  7.4336  &  7.9159  &  8.2942 \\
       \hline 
       & $\sigma_M$ & 0.7292  &  0.8920  & 1.2704  &  1.8695  &  2.4069 \\
       \hline
    \end{tabular}
  \end{center}
  \caption{\label{tab:low-mass-parametric} Quantiles of the
    marginalized distribution for each of the parameters in the models
    discussed in Section \ref{sec:parametric-models}.  We indicate
    the 5\%, 15\%, 50\% (median), 85\%, and 95\% quantiles.  The
    marginalized distribution can be misleading when there are strong
    correlations between variables.  For example, while the
    marginalized distributions for the power law parameters are quite
    broad, the distribution of mass distributions implied by the power
    law MCMC samples is similar to the other models.  This occurs in
    spite of the broad marginalized distributions because of the
    correlations between the slope and limits of the power law
    discussed in Section \ref{sec:power-law}.}
\end{table}

\subsubsection{Power-Law Models}
\label{sec:power-law}

Many astrophysical distributions are power laws.  Let us assume that
the BH mass distribution is given by
\begin{equation}
  \label{eq:power-law-dist}
  p(M|\vtheta) = p(M|\{\Mmin, \Mmax, \alpha \}) =
  \begin{cases}
    A M^\alpha & \Mmin \leq m \leq \Mmax \\
    0 & \textnormal{otherwise}
  \end{cases}.
\end{equation}
The normalizing constant $A$ is given by 
\begin{equation}
  A = \frac{1+\alpha}{\Mmax^{1+\alpha} - \Mmin^{1+\alpha}}.
\end{equation}
We choose uniform priors on $\Mmin$ and $\Mmax \geq \Mmin$ between 0 and
$40 \Msun$, and uniform priors on the exponent $\alpha$ in a broad
range between $-15$ and $13$:
\begin{equation}
  p(\vtheta) = p(\{\Mmin, \Mmax, \alpha\}) = 
  \begin{cases}
    2 \frac{1}{40^2}\frac{1}{28} & 0 \leq \Mmin \leq \Mmax
    \leq 40, \quad -15 \leq \alpha \leq 13 \\
    0 & \textnormal{otherwise}
  \end{cases}.
\end{equation}

Our MCMC analysis output is a list of $\{\Mmin, \Mmax, \alpha\}$
values distributed according to the posterior 
\begin{equation}
  p(\vtheta|d) = p(\{\Mmin, \Mmax, \alpha\}|d) \propto p(d|\{\Mmin,
  \Mmax, \alpha\}) p(\{\Mmin, \Mmax, \alpha\}),
\end{equation}
with the likelihood $p(d|\{\Mmin, \Mmax, \alpha\})$ defined in
Equation \eqref{eq:likelihood-def}.  Figure \ref{fig:dists} presents
the resulting distribution of mass distributions inferred from the
parameters output by the MCMC.

In Figure \ref{fig:power-law} , we display a histogram of the resulting
samples in each of the parameters $\Mmin$, $\Mmax$, and $\alpha$
individually; this represents the one-dimensional ``marginalized''
distribution
\begin{equation}
  \label{eq:alpha-pdf}
  p(\alpha|d) = \int d\Mmin\, d\Mmax\, p(\{\Mmin, \Mmax, \alpha\}|d),
\end{equation}
and similarly for $\Mmin$ and $\Mmax$.

\begin{figure}
  \begin{center}
    \plotone{plots/power-law}
  \end{center}
  \caption{\label{fig:power-law} A histogram of the marginalized
    distribution for the three parameters $\Mmin$, $\Mmax$, and
    $\alpha$ from the power-law model.  The marginalized distribution
    for $\alpha$ is broad, with $-11.8 < \alpha < 6.8$ enclosing 90\%
    of the probability.  We have $p(\alpha < 0) = 0.6$; the median
    value is $\alpha = -3.35$.  The broad distribution for $\alpha$
    (and the other parameters) is due to correlations between the
    parameters discussed in the main text; see Figure
    \ref{fig:power-law-2D}.}
\end{figure}

The marginalized distribution for $\alpha$ is broad, with
\begin{equation}
  -11.8 < \alpha < 6.8
\end{equation}
enclosing 90\% of the probability (excluding 5\% on each side).  We
have $p(\alpha < 0) = 0.6$.  The median value is $\alpha = -3.35$.
The broadness of the marginalized distribution for $\alpha$ comes from
the need to match the relatively narrow range in mass of the
lower-mass systems.  When $\alpha$ is negative, the resulting mass
distribution slopes down; $\Mmin$ is constrained to be near the lowest
mass of the observed black holes, while $\Mmax$ is essentially
irrelevant.  Conversely, when $\alpha$ is positive and the mass
distribution slopes up, $\Mmax$ must be close to the largest mass
observed, while $\Mmin$ is essentially irrelevant.  Figure
\ref{fig:power-law-2D} illustrates this effect, showing the
correlations between $\alpha$ and $\Mmin$ and $\alpha$ and $\Mmax$.
When we include the higher-mass systems in the analysis, the long tail
will eliminate this effect by bringing both $\Mmin$ and $\Mmax$ into
play for all values of $\alpha$.

\begin{figure}
  \begin{center}
    \plotone{plots/power-law-2D}
  \end{center}
  \caption{\label{fig:power-law-2D} MCMC samples in the $\Mmin,
    \alpha$ (top) and $\Mmax, \alpha$ (bottom) planes for the
    power-law model discussed in Section \ref{sec:power-law}.  The
    correlations between $\alpha$ and the power law bounds discussed
    in the text are apparent: when $\alpha$ is positive, the mass
    distribution slopes upward and $\Mmax$ is constrained to be near
    the maximum observed mass while $\Mmin$ is unconstrained.  When
    $\alpha$ is negative, the mass distribution slopes down and
    $\Mmin$ is constrained to be near the lowest mass observed, while
    $\Mmax$ is unconstrained. }
\end{figure}

\subsubsection{Decaying Exponential}
\label{sec:exponential}

\citet{Fryer2001} suggest that the black-hole mass distribution may be
well-represented by a decaying exponential with a minimum mass:
\fixme{Vicky, do you want to say anything more here?}
\begin{equation}
  \label{eq:exp-def}
  p(M|\vtheta) = p(M|\{\Mmin, M_0\}) = 
  \begin{cases}
    \frac{e^{\frac{\Mmin}{M_0}}}{M_0} \exp \left[ - \frac{M}{M_0}
    \right] & M \geq \Mmin \\
    0 & \textnormal{otherwise}
  \end{cases}.
\end{equation}
We choose a prior for this model where $\Mmin$ is uniform between 0
and $40 \Msun$.  For each $\Mmin$, we choose $M_0$ uniformly within a
range ensuring that $40\Msun$ is at least two scale masses above the
cutoff: $40\Msun \geq \Mmin + 2 M_0$.  This ensures that the majority
of the mass probability lies in the range $0 \leq M \leq 40\Msun$.
The resulting prior is
\begin{equation}
  p(\vtheta) = p(\{\Mmin, M_0\}) = 
  \begin{cases}
    \frac{4}{40^2} & 0 \leq \Mmin \leq 40, \quad 0 < M_0, \quad \Mmin
    + 2 M_0 \leq 40, \\
    0 & \textnormal{otherwise}
  \end{cases}
\end{equation}

Figure \ref{fig:exp-marginal} displays the marginalized posterior
distribution for the scale mass of the exponential, $M_0$, and the
cutoff mass, $\Mmin$.  The median scale mass is $M_0 = 1.55$, and
$0.78 \leq M_0 \leq 3.25$ with 90\% confidence.  Figure
\ref{fig:exp-2D} displays the MCMC samples in the $\Mmin$, $M_0$ plane
for this model.  There is a small correlation between smaller $\Mmin$
and larger $M_0$, which is driven by the need to widen the
distribution to encompass the peak of the mass measurements in Figure
\ref{fig:low-masses} when the minimum mass is smaller.

\begin{figure}
  \begin{center}
    \plotone{plots/exp-cutoff}
  \end{center}
  \caption{\label{fig:exp-marginal} The distribution of scale masses,
    $M_0$, and minimum masses, $\Mmin$, both measured in units of a
    solar mass for the exponential underlying mass distribution
    defined in Equation \eqref{eq:exp-def}.  The median scale mass is
    $M_0 = 1.55$, and $0.78 \leq M_0 \leq 3.25$ with 90\%
    confidence.}
\end{figure}

\begin{figure}
  \begin{center}
    \plotone{plots/exp-cutoff-2d}
  \end{center}
  \caption{\label{fig:exp-2D} The MCMC samples in the $\Mmin$, $M_0$
    plane for the decaying exponential underlying mass distribution
    model.  The slight correlation between smaller $\Mmin$ and larger
    $M_0$ is driven by the need to widen the mass distribution to
    encompass the peak of the measurements in Figure
    \ref{fig:low-masses} when the minimum mass decreases.}
\end{figure}

\subsubsection{Gaussian and Two-Gaussian Models}
\label{sec:gaussian}

The mass distribution in Figure \ref{fig:low-masses} appears to have a
single peak.  The prototypical single-peaked probability distribution
is a Gaussian:
\begin{equation}
  \label{eq:gaussian-def}
  p(M|\vtheta) = p(M|\{\mu, \sigma\}) = \frac{1}{\sigma \sqrt{2\pi}}
  \exp\left[ - \left(\frac{M - \mu}{\sqrt{2} \sigma} \right)^2 \right].
\end{equation}
We use a prior on the mean mass, $\mu$, and the standard deviation,
$\sigma$, that ensures that the majority of the mass distribution lies
below $40 \Msun$:
\begin{equation}
  \label{eq:gaussian-prior-def}
  p(\{\mu,\sigma\}) = 
  \begin{cases}
    \frac{8}{40^2} & 0 \leq \mu \leq 40, \quad \sigma \geq 0, \quad
    \mu + 2\sigma \leq 40 \\
    0 & \textnormal{otherwise}
  \end{cases},
\end{equation}
where both $\mu$ and $\sigma$ are measured in solar masses.  Figure
\ref{fig:gaussian} shows the resulting marginalized distributions for
the parameters $\mu$ and $\sigma$.  We constrain the peak of the
Gaussian between $6.63 \leq \mu \leq 8.08$ with 90\% confidence.
(Refer to Figure \ref{fig:dists} for the shape of the resulting
distribution of distributions.)

\begin{figure}
  \begin{center}
    \plotone{plots/gaussian}
  \end{center}
  \caption{\label{fig:gaussian} Marginalized posterior distributions
    for the mean $\mu$ and standard deviation $\sigma$ (both in solar
    masses) for the Gaussian underlying mass distribution defined in
    Equation \eqref{eq:gaussian-def}.  The peak of the Gaussian,
    $\mu$, is constrained in $6.63 \leq \mu \leq 8.08$ with 90\%
    confidence.}
\end{figure}

Though we do not expect to find a second peak in the low-mass
distribution, we may find evidence of one when exploring the combined
low- and higher-mass samples.  To look for a second peak in the
black-hole mass distribution, we use a two-Gaussian model:
\begin{multline}
  \label{eq:two-gaussian-def}
  p(M|\vtheta) = p(M|\{\mu_1, \mu_2, \sigma_1, \sigma_2, \alpha\}) = \\
  \frac{\alpha}{\sigma_1 \sqrt{2\pi}} \exp\left[ - \left( \frac{M -
        \mu_1}{\sqrt{2}\sigma_1} \right)^2 \right] + \frac{1-\alpha}{\sigma_2 \sqrt{2\pi}} \exp\left[ - \left( \frac{M -
        \mu_2}{\sqrt{2}\sigma_2} \right)^2 \right].
\end{multline}
The probability is a linear combination of two Gaussians with weight
$\alpha$.  We restrict $\mu_1 < \mu_2$ and also impose combined
conditions on $\mu_i$ and $\sigma_i$ that ensure that most of the mass
probability lies below $40 \Msun$ with the prior 
\begin{equation}
  p(\{\mu_1, \mu_2, \sigma_1, \sigma_2, \alpha\}) = 
  \begin{cases}
    2 p(\{\mu_1, \sigma_1\}) p(\{\mu_2, \sigma_2\}) & \mu_1 \leq
    \mu_2, \quad 0 \leq \alpha \leq 1 \\
    0 & \textnormal{otherwise}
  \end{cases},
\end{equation}
where the single-Gaussian prior, $p(\{\mu_i, \sigma_i\})$, is defined
in Equation \eqref{eq:gaussian-prior-def}.

Figure \ref{fig:two-gaussian} shows the marginalized distributions for
the two-Gaussian model parameters from our MCMC runs.  We find $\alpha
> 0.8$ with 62\% probability, clearly favoring the first Gaussian.
The distribution for $\mu_1$ and $\sigma_1$ are similar to those of
the single Gaussian displayed in Figure \ref{fig:gaussian}, indicating
that the first Gaussian is centered around the peaks of the lower-mass
distributions.  The second Gaussian's parameter distributions are much
broader.  The second Gaussian appears to be sampling the tail of the
mass samples.  In spite of the ability to match a more complicated
distribution, we find that this model is strongly disfavored relative
to the single-Gaussian model for this dataset:
$p(\textnormal{Gaussian}|d) / p(\textnormal{Two Gaussian}|d) \simeq
4.7$ (see Section \ref{sec:model-selection} for discussion).

\begin{figure}
  \begin{center}
    \plotone{plots/two-gaussian}
  \end{center}
  \caption{\label{fig:two-gaussian} The marginal distributions for the
    five parameters of the two-Gaussian model.  We have $\alpha > 0.8$
    with 62\% probability, favoring the first of the two Gaussians.
    The distributions for $\mu_1$ and $\sigma_1$ are similar to those
    of the single Gaussian model displayed in Figure
    \ref{fig:gaussian}; the second Gaussian's parameter distributions
    are much broader (recall that we constrain $\mu_2 > \mu_1$).  The
    second Gaussian is attempting to fit the tail of the mass samples.
    The extra degrees of freedom in the distribution from the second
    Gaussian do not provide enough extra fitting power to compensate
    for the increase in parmeter space, however: the two-Gaussian
    model is disfavored relative to the single Gaussian by a factor of
    $4.7$ on this dataset (see Section \ref{sec:model-selection} for
    discussion).}
\end{figure}

\subsubsection{Log Normal}
\label{sec:log-normal}

Many of the mass distributions for the systems in Figure
\ref{fig:low-masses} rise rapidly to a peak and then fall off more
slowly in a longer tail toward high masses.  So far, none of the
parameterized distributions we have discussed have this property.  In
this section, we consider a log-normal model for the underlying mass
distribution; the log-normal distribution has a rise to a peak with a
slower falloff in a long tail.  

The log-normal distribution gives $\log M$ a Gaussian distribution
with mean $\mu$ and standard deviation $\sigma$:
\begin{equation}
  \label{eq:log-normal-def}
  p(M|\vtheta) = p(M|\{\mu, \sigma \}) = \frac{1}{
    \sqrt{2\pi} M \sigma} \exp\left[ -\frac{\left(\log M - \mu\right)^2}{2 \sigma^2} \right].
\end{equation}
The parameters $\mu$ and $\sigma$ are dimensionless; the mean mass
$\langle M \rangle$ and mass standard deviation $\sigma_M$ are related
to $\mu$ and $\sigma$ by
\begin{eqnarray}
  \label{eq:avg-M}
  \langle M \rangle & = & \exp\left( \mu + \frac{1}{2} \sigma^2
  \right) \\
  \label{eq:sigma-M}
  \sigma_M & = & \langle M \rangle \sqrt{\exp\left( \sigma^2 \right) - 1}.
\end{eqnarray}
For a fair comparison with the other models, we impose a prior that is
flat in $\langle M \rangle$ and $\sigma_M$.  To ensure that most of
the probability in this model occurs for masses below $40 \Msun$, we
require $\langle M \rangle + 2 \sigma_M \leq 40$, resulting in a
prior
\begin{equation}
  p(\vtheta) = p(\{\mu,\sigma\}) = 
  \begin{cases}
    \frac{4}{40^2} \left| \frac{\partial \left(\langle M \rangle,
          \sigma_M \right)}{\partial \left( \mu, \sigma \right)}
    \right| & \sigma > 0, \quad \langle M \rangle + 2 \sigma_M \leq 40
    \\
    0 & \textnormal{otherwise}
  \end{cases},
\end{equation}
where 
\begin{equation}
  \left| \frac{\partial \left(\langle M \rangle,
          \sigma_M \right)}{\partial \left( \mu, \sigma \right)}
    \right| = \frac{\exp\left( 2 \left( \mu + \sigma^2 \right)\right)
      \sigma}{\sqrt{\exp\left( \sigma^2 \right) - 1}}
\end{equation}
is the determinant of the Jacobian of the map in Equations
\eqref{eq:avg-M} and \eqref{eq:sigma-M}.

The marginal distributions for $\langle M \rangle$ and $\sigma_M$
appear in Figure \ref{fig:log-normal}.  The distributions are similar
to those for $\mu$ and $\sigma$ from the Gaussian model in Section
\ref{sec:gaussian}.

\begin{figure}
  \begin{center}
    \plotone{plots/log-normal}
  \end{center}
  \caption{\label{fig:log-normal} Marginalized distributions of the
    mean mass, $\langle M \rangle$, and standard deviation of the
    mass, $\sigma_M$, for the log-normal model in Section
    \ref{sec:log-normal}.  The distributions are similar to the
    distributions of $\mu$ and $\sigma$ in the Gaussian model of
    Section \ref{sec:gaussian}.}
\end{figure}

\subsection{Non-Parametric Models for the Black Hole Mass Distribution}
\label{sec:non-parametric-models}

The previous subsection discussed models for the underlying black hole
mass distribution that assumed particular parameterized shapes for the
distribution.  In this subsection, we will discuss models that do not
assume a priori a shape for the black hole mass distribution.  The
fundamental non-parametric distribution in this section is a
histogram with some number of bins, $\Nbin$.  Such a distribution is
piecewise-constant in $M$.

One choice for representing such a histogram would be to fix the bin
locations, and allow the heights to vary.  With this approach, one
should be careful not to ``split'' features of the mass distribution
across more than one bin in order to avoid diluting the sensitivity to
such features; similarly, one should avoid including more than ``one''
feature in each bin.  The locations of the bins, then, are crucial.
An alternative representation of histogram mass distributions avoids
this difficulty.

We choose to represent a histogram mass distribution with $\Nbin$ bins
by allocating a fixed probability, $1/\Nbin$, to each bin.  The lower
and upper bounds for each bin are allowed to vary; when these are
close to each other (i.e.\ the bin is narrow), the distribution will
have a large value, and conversely when the bounds are far from each
other.  We assume that the non-zero region of the distribution is
contiguous, so we can represent the boundaries of the bins as a
non-decreasing array of masses, $w_0 \leq w_1 \leq \ldots \leq
w_{\Nbin}$, with $w_0$ the minimum and $w_{\Nbin}$ the maximum mass
for which the distribution has support.  This gives the distribution
\begin{equation}
  \label{eq:hist-def}
  p(M|\theta) = p(M|\{w_0, \ldots, w_{\Nbin}\}) = 
  \begin{cases}
    0 & M < w_0 \textnormal{ or } w_{\Nbin} \leq M \\
    \frac{1}{\Nbin} \frac{1}{w_{i+1} - w_i} & w_i \leq M < w_{i+1}
  \end{cases}.
\end{equation}

For priors on the histogram model with $\Nbin$ bins, we assume that
the bin boundaries are uniformly distributed between 0 and $40 \Msun$
subject only to the constraint that the boundaries are non-decreasing
from $w_0$ to $w_{\Nbin}$:
\begin{equation}
  p(\{w_0, \ldots, w_{\Nbin}\}) = 
  \begin{cases}
    \frac{\left(\Nbin+1\right)!}{40^{\Nbin+1}} & 0 \leq w_0 \leq w_1
    \leq \ldots \leq w_{\Nbin} \leq 40 \\
    0 & \textnormal{otherwise}
  \end{cases}.
\end{equation}

The median values of the histogram mass distributions that result from
the MCMC samples of the posterior distribution for the $w_i$
parameters for one-, two-, three-, four-, and five-bin histogram
models are shown in Figure \ref{fig:dists}.  Table
\ref{tab:low-mass-non-parametric} gives quantiles of the marginalized
bin boundary distributions for the histogram models.

\begin{table}
  \begin{center}
    \begin{tabular}{|c|c|c|c|c|c|c|}
      \hline
      Bins & Boundary & 5\% & 15\% & 50\% & 85\% & 95\% \\
      \hline \hline
      1 & $w_0$ & 3.94488 & 4.55603 & 5.43333 & 6.02557 & 6.29749 \\
      \hline
        & $w_1$ & 8.50844 & 8.69262 & 9.11784 & 9.83477 & 10.5128 \\
      \hline \hline
      2 & $w_0$ & 3.3426 & 4.2047 & 5.39132 & 6.18413 & 6.47553 \\
      \hline
        & $w_1$ & 6.41972 & 6.72605 & 7.43421 & 8.2489 & 8.52885 \\
      \hline
        & $w_2$ & 8.46161 & 8.65077 & 9.12694 & 10.1113 & 11.2595 \\
      \hline \hline
      3 & $w_0$ & 2.18176 & 3.54345 & 5.16094 & 6.16473 & 6.44697 \\
      \hline
        & $w_1$ & 5.68876 & 6.14223 & 6.68829 & 7.38725 & 8.04235 \\
      \hline
        & $w_2$ & 6.8297 & 7.22718 & 8.1451 & 8.7512 & 9.27296 \\
      \hline
        & $w_3$ & 8.44307 & 8.67362 & 9.25718 & 12.1688 & 21.92 \\
      \hline \hline
      4 & $w_0$ & 1.32131 & 2.7934 & 4.66156 & 5.78459 & 6.17946 \\
      \hline
        & $w_1$ & 5.20112 & 5.77331 & 6.42501 & 6.98427 & 7.44584 \\
      \hline
        & $w_2$ & 6.41805 & 6.73535 & 7.43826 & 8.32958 & 8.64212 \\
      \hline
        & $w_3$ & 7.40302 & 7.95608 & 8.58976 & 9.33897 & 10.3992 \\
      \hline
        & $w_4$ & 8.56724 & 8.8059 & 10.2451 & 24.3573 & 34.2423 \\
      \hline \hline
      5 & $w_0$ & 0.9392 & 2.28789 & 4.33389 & 5.7012 & 6.21166 \\
      \hline
        & $w_1$ & 4.69778 & 5.44302 & 6.26575 & 6.76407 & 7.14427 \\
      \hline
        & $w_2$ & 6.1388 & 6.47155 & 7.00606 & 7.97325 & 8.38259 \\
      \hline
        & $w_3$ & 6.82058 & 7.28677 & 8.22514 & 8.81555 & 9.41012 \\
      \hline
        & $w_4$ & 8.02335 & 8.36993 & 8.94879 & 11.3206 & 17.3349 \\
      \hline
        & $w_5$ & 8.7112 & 9.25208 & 16.2059 & 31.897 & 37.2738 \\
      \hline
    \end{tabular}
  \end{center}
  \caption{\label{tab:low-mass-non-parametric} The 5\%, 15\%, 50\%
    (median), 85\%, and 95\% quantiles for the bin boundaries in the
    one- through five-bin histogram models discussed in Section
    \ref{sec:non-parametric-models}.}
\end{table}

As the number of bins increases, the models are better able to capture
features of the mass distribution, but we find that the one-bin
histogram is the most probable of the histogram models for the
low-mass data (see Section \ref{sec:model-selection} for discussion).
This occurs because the extra fitting power does not sufficiently
improve the fit to compensate for the vastly larger parameter space of
the models with more bins.

\section{Model Selection}
\label{sec:model-selection}

In Section \ref{sec:models}, we discussed a series of models for the
underlying black hole mass distribution.  For each model, we have
assumed that the underlying mass distribution corresponds to the
model, and we ask what distributions are implied by the data for the
parameters in the model.  We have not yet asked which models are more
likely to correspond to the actual distribution.  That is the topic of
this section.

To compare models in the context of a Bayesian analysis, consider a
set of models, $\{M_i| i = 1, \ldots\}$, each with corresponding
parameters $\vtheta_i$.  Re-writing Equation \eqref{eq:Bayes-rule} to
be explicit about the assumption of a particular model, we have
\begin{equation}
  p(\vtheta_i | d, M_i) = \frac{p(d|\vtheta_i, M_i) p(\vtheta_i | M_i)}{p(d|M_i)}.
\end{equation}
This gives the posterior probability of the parameters $\vtheta_i$ in
the context of model $M_i$.  But, the model itself can be regarded as
a discrete parameter in a larger ``super-model'' that encompasses all
the $M_i$.  The parameters for the super-model are $\{M_i,
\vtheta_i\}$: a choice of model and the corresponding parameter value
within that model.  The posterior probability of the super-model
parameters is given by Bayes' rule:
\begin{equation}
  \label{eq:bayes-explicit-model}
  p(\vtheta_i, M_i|d) = \frac{p(d|\vtheta_i, M_i) p(\vtheta_i |M_i) p(M_i)}{p(d)},
\end{equation}
where we have introduced the model prior $p(M_i)$, which represents
our estimate on the probability that model $M_i$ is correct in the
absence of the data $d$.  The normalizing evidence is now
\begin{equation}
  \label{eq:multi-model-evidence-def}
  p(d) = \sum_i \int d\vtheta_i\, p(\vtheta_i, M_i|d) = \sum_i
  p(d|M_i) p(M_i),
\end{equation}
writing the single-model evidence from Equation
\eqref{eq:evidence-def} as $p(d|M_i)$ to be explicit about the
dependence on the choice of model.

To compare the various models $M_i$, we are interested in the
marginalized posterior probability of $M_i$:
\begin{equation}
  \label{eq:model-posterior-def}
  p(M_i|d) \equiv \int d\vtheta_i\, p(\vtheta_i, M_i|d).
\end{equation}
This is the integral of the posterior over the entire parameter space
of model $M_i$.  The marginalized posterior probability of model $M_i$
can be re-written in terms of the single-model evidence, $p(d|M_i)$
(see Equations \eqref{eq:bayes-explicit-model} and
\eqref{eq:evidence-def}):
\begin{equation}
  \label{eq:model-evidence-def}
  p(M_i|d) = \int d\vtheta_i\, p(\vtheta_i, M_i|d) = \frac{p(M_i)}{p(d)} \int d\vtheta_i
  p(d|\vtheta_i,M_i) p(\vtheta_i|M_i) = \frac{p(d|M_i) p(M_i)}{p(d)}.
\end{equation}

Here and throughout, we assume that any of the models in Section
\ref{sec:models} are equally likely a priori, so the model priors are
equal:
\begin{equation}
  p(M_i) = \textnormal{const}.
\end{equation}

\subsection{Reversible-Jump MCMC}
\label{sec:reversible-jump-mcmc}

A powerful technique for computing $p(M_i|d)$ is the reversible-jump
MCMC \citep{Green1995}.  Consider the ``super-model'' that encompasses
all the models under consideration.  The parameter space of the
super-model consists of a discrete parameter that identifies the
choice of model, $M_i$, and the continuous parameters appropriate for
this model, $\vtheta_i$.  We denote a point in the super-model
parameter space by $\{M_i, \vtheta_i\}$; each such point is a
statement that, e.g., ``the underlying mass distribution is a
Gaussian, with parameters $\mu$ and $\sigma$,'' or ``the underlying
mass distribution is a triple-bin histogram with parameters $w_1$,
$w_2$, $w_3$, and $w_4$,'' or ....  To compare models, we are
interested in the quantity (see Equation
\eqref{eq:model-posterior-def})
\begin{equation}
  p(M_i|d) = \int d\vtheta_i p(M_i, \vtheta_i|d).
\end{equation}
If we perform an MCMC in the super-model parameter space, then we
obtain a chain of samples $\{M_i, \vtheta_i\}$ distributed in
parameter space with density $p(M_i,\vtheta_i|d) d\vtheta_i$ and we can
estimate the integral as
\begin{equation}
  p(M_i|d) = \int d\vtheta_i p(M_i, \vtheta_i|d) \approx \frac{N_i}{N},
\end{equation}
where $N_i$ is the number of MCMC samples that have discrete parameter
$M_i$, and $N$ is the total number of samples in the MCMC.  In other
words, the fraction of samples lying in the parameter space of model
$M_i$ gives the probability of that model relative to the other models
under consideration.

To perform the MCMC in the super-model parameter space, we must
propose jumps not only between points in a particular model's
parameter space (as we have been doing up to now), but also between
the parameter spaces of different models.  For this MCMC to be
efficient, proposed jumps into a model from another should favor
regions with large posterior; when the posterior is highly-peaked in a
small region of parameter space, proposed jumps outside this region
are unlikely to be accepted, and the reversible-jump MCMC samples will
require a very long chain to properly sample the ``super-model''
posterior.  

We can exploit the information we have from the single-model MCMCs
described in Section \ref{sec:models} to generate efficient jump
proposal distributions for our reversible jump MCMC.  We would like to
propose jumps that roughly follow the distribution of samples in the
single-model MCMCs.  We can do this by assigning a neighborhood to
each point in the sample using an algorithm we will describe in the
following paragraphs; the neighborhoods are non-overlapping,
completely cover the region of parameter space with prior support, and
contain only one point from the MCMC samples.  To propose a jump into
model $M_i$, we choose a point uniformly from that single-model MCMC
and then propose a jump drawn uniformly from that point's
neighborhood.  This is equivalent to drawing from a piecewise-constant
approximation to the single-model posterior, where each neighborhood
contributes a constant fraction, $1/N_i$, to the cumulative jump
probability.  In regions of high density the neighborhoods are
smaller, and the jump probability density is correspondingly higher.
Because the neighborhoods cover the entire region of prior support, it
is possible for the proposal to propose any point in parameter space
with prior support (though points in regions of low single-model
posterior are of course unlikely to be proposed).

To assign a neighborhood to each point in a single-model MCMC we use a
data structure called a kD-tree.  A kD-tree is a binary
space-partitioning tree.  To construct a kD-tree, we begin with the
set of points from a single-model MCMC and a box in parameter space
bounding the region of prior support (which must necessarily enclose
all the points).  The construction proceeds recursively: we choose a
dimension%
\footnote{Our algorithm chooses the dimension along which the
  numerical extent of the points is largest.  Other choices are
  possible; some algorithms cycle through the dimensions in order,
  while others choose a random dimension for each subdivision.  Our
  goal by picking the longest dimension is to produce neighborhoods
  that are ``square,'' at least in the chosen parametrization.} %
along which to divide the points, find the median point along that
dimension and its nearest neighbor, and divide the box at the midpoint
between these two points, producing two sub-boxes.  We then partition
the points into those to the left (i.e.\ smaller coordinate along the
given dimension) and right of the dividing line, and repeat this
procedure for each subset and the corresponding bounding box, until we
have only one point in each box.  An example of the neighborhoods that
result from a two-dimensional kD-tree constructed around a Gaussian
point distribution appears in Figure \ref{fig:kD-tree}.

\begin{figure}
  \begin{center}
    \plotone{plots/kdtree}
  \end{center}
  \caption{\label{fig:kD-tree} The neighborhoods constructed from a
    two-dimensional kD-tree built from a sample of points with a
    Gaussian density distribution.  Each line on the figure
    corresponds to a sub-dividing box boundary drawn between the
    median of a subset of the sample points and its nearest
    neighbor. The peak of the Gaussian lies in the center of the
    figure; here the point density is highest and the neighborhoods
    are smallest.  Near the edges the density is lower, and the
    neighborhoods correspondingly larger.  The tree adapts itself to
    the local density of points.  If these were single-model MCMC
    samples, the corresponding jump proposal would first select one of
    the boxes uniformly at random, and then choose a point uniformly
    within the box to propose.  Since there are many more boxes near
    the center (each box corresponds to one point), and these boxes
    are smaller, the proposal will tend to concentrate its points
    there, approximately tracking the distribution of single-model
    MCMC samples.}
\end{figure}

Construction of a kD-tree is an $\order{N\log N}$ operation, where $N$
is the number of points in the tree.  Median finding is $\order{n}$,
where $n$ is the number of points from which the median is to be
obtained.  At level $i$ in the tree, there are $2^i$ subsets of
points, each of length $\order{N/2^i}$, so the total cost of the $2^i$
median calculations is $\order{N}$ at each level.  There are
$\order{\log N}$ levels in the tree, yielding a total construction
cost for the tree of $\order{N \log N}$.  

To find the neighborhood of a point using the tree, we begin at the
root of the tree, and examine the two sub-boxes at the next level
down.  The point will be in one of them; following that branch, we
have again two sub-boxes, one of which contains the point; following
that branch....  Eventually, the search terminates at a leaf of the
tree, containing the point in question.  The box at the leaf defines
the neighborhood of the point in the jump proposal algorithm described
above.  The total cost for this operation is proportional to the
number of levels in the tree, which is $\order{\log N}$.

We have performed a suite of 500 independent reversible-jump MCMCs
jumping between all the models (both parametric and non-parametric) in
Section \ref{sec:models} using the single-model MCMC samples to
construct an efficient jump proposal for each model as described
above.  The numbers of counts in each model are consistent across the
MCMCs in the suite; Figure \ref{fig:rj} displays the average
probability for each model across the suite, along with the 1-$\sigma$
errors on the average inferred from the standard deviation of the
model counts across the suite.  Table \ref{tab:rj} gives the numerical
values of the average probability for each model across the suite of
MCMCs.

\begin{figure}
  \begin{center}
    \plotone{plots/rj}
  \end{center}
  \caption{\label{fig:rj} The relative probability of the models
    discussed in Section \ref{sec:models} as computed using the
    reversible-jump MCMC with the efficient jump proposal algorithm
    described in Section \ref{sec:reversible-jump-mcmc}.  In
    increasing order along the $x$-axis, the models are the power-law
    of Section \ref{sec:power-law} (PL), the decaying exponential of
    Section \ref{sec:exponential} (E), the single Gaussian of Section
    \ref{sec:gaussian} (G), the double Gaussian of Section
    \ref{sec:gaussian} (TG), and the one-, two-, three-, four-, and
    five-bin histogram models of Section
    \ref{sec:non-parametric-models} (H1, H2, H3, H4, H5,
    respectively).  The average of 500 independent reversible-jump
    MCMCs is plotted, along with the 1-$\sigma$ error on the average
    inferred from the standard deviation of the probability from the
    individual MCMCs.  As discussed in the text, the power-law and
    Gaussian models are the most favored.}
\end{figure}

The most favored model is the power law from Section
\ref{sec:power-law}, followed by the Gaussian model from Section
\ref{sec:gaussian}.  This confirms the results from the harmonic mean
evidence estimator in the last subsection.  (Though note that the
harmonic mean ranks the one-bin histogram significantly higher than
the reversible jump calculation; given the large uncertainty in the
harmonic mean estimator, it is likely that the true evidence for the
one-bin histogram model is closer to the value computed in this
section.)  Interestingly, the theoretical curve from \citet{Fryer2001}
(the exponential model of Section \ref{sec:exponential}) places fourth
in the ranking of evidence.  

\begin{table}
  \begin{center}
    \begin{tabular}{|l|r|}
      \hline
      Model & Relative Evidence \\
      \hline \hline
      Power Law (Section \ref{sec:power-law}) & 0.331488 \\
      \hline
      Gaussian (Section \ref{sec:gaussian}) & 0.288129 \\ 
      \hline
      Log Normal (Section \ref{sec:log-normal}) & 0.138435 \\
      \hline
      Exponential (Section \ref{sec:exponential}) & 0.0916218 \\
      \hline
      Two Gaussian (Section \ref{sec:gaussian}) & 0.0662577 \\
      \hline
      Histogram (1 Bin, Section \ref{sec:non-parametric-models}) &
      0.0641941 \\
      \hline
      Histogram (2 Bin, Section \ref{sec:non-parametric-models}) &
      0.015184 \\
      \hline 
      Histogram (3 Bin, Section \ref{sec:non-parametric-models}) &
      0.00332933 \\
      \hline
      Histogram (4 Bin, Section \ref{sec:non-parametric-models}) &
      0.000999976 \\
      \hline 
        Histogram (5 Bin, Section \ref{sec:non-parametric-models}) &
      0.0003614  \\
      \hline      
    \end{tabular}
  \end{center}
  \caption{\label{tab:rj} Relative probabilities of the various models
    from Section \ref{sec:models} computed from reversible-jump
    MCMC samples with the efficient jump proposal algorithm in Section \ref{sec:reversible-jump-mcmc}.}
\end{table}

Though the model probabilities presented in this section have small
statistical error, they are subject to large ``systematic'' error.
The source of this error is both the particular choice of model prior
(uniform across models) and the choice of priors on the parameters
within each model used for this work.  For example, the
theoretically-preferred exponential model (Section
\ref{sec:exponential}) is only a factor of $\sim 3$ away from the
power law model (Section \ref{sec:power-law}), which does not have
theoretical support.  Is such support worth a factor of three in the
model prior?  Alternately, we may say we know (in advance of any mass
measurements) that black holes must exist with mass $\lesssim
10\Msun$; then we could, for example, impose a prior on the minimum
mass in the exponential model ($\Mmin$) that is uniform between $0$
and $10 \Msun$, which would reduce the prior volume available for the
model by a factor of 4 without significantly reducing the posterior
support for the model.  This has the same effect as increasing the
model prior by a factor of 4, which would move this model from fourth
to first place.  Of course, we would then have to modify the prior
support for the other models to take into account the restriction that
there must be black holes with $M \lesssim 10\Msun$....
\citet{Linder2008} discuss these issues in the context of cosmological
model selection, concluding with a similar warning against
over-reliance on model selection probabilities.

Nevertheless, we believe that our model comparison is reasonably fair
(see the discussion of priors in Section \ref{sec:priors}).  It seems
safe to conclude that ``single-peaked'' models (the power-law and
Gaussian) are preferred over ``extended'' models (the exponential or
log-normal), or those with ``structure'' (the many-bin histograms or
two-Gaussian model).  Previous studies have also supported the
``single, narrow peak'' mass distribution \citep{Bailyn1998,Ozel2010}.
In this light, poor performance of the single-bin histogram is
surprising.

\section{Including High-Mass, Wind-Fed Systems}
\label{sec:higher-mass}

This section repeats the analysis of Sections \ref{sec:models} and
\ref{sec:model-selection} but including the higher-mass, wind-fed
systems from Table \ref{tab:sources} (see also Figure
\ref{fig:high-masses}) in the sample.  Figure
\ref{fig:high-mass-dists} displays bounds on the value of the
underlying mass distribution for the various models in Section
\ref{sec:models} applied to this data set; compare to Figure
\ref{fig:dists}.  The inclusion of the higher-mass, wind-fed systems
tends to widen the distribution toward the high-mass end and, in
models that allow it, produce a second, higher-mass peak in addition
to the one in Figure \ref{fig:dists}.

\begin{figure}
  \begin{center}
    \plottwo{plots/dist-parametric-high}{plots/dist-non-parametric-high-mass}
  \end{center}
  \caption{\label{fig:high-mass-dists} The median values of the black
    hole mass distribution, $p(M|\theta)$, at various masses implied
    by the posterior $p(\theta|d)$ for the models discussed in
    Sections \ref{sec:parametric-models} and
    \ref{sec:non-parametric-models}.  These distributions use the
    complete sample of 20 observations in Table \ref{tab:sources},
    including the higher-mass, wind-fed systems.  Error bars span the
    10\% to 90\% range.  Note that these ``distributions of
    distributions'' are not necessarily normalized, and need not be
    ``shaped'' like the underlying model distributions.  Compare to
    Figure \ref{fig:dists}, which includes only the low-mass systems
    in the analysis.  Including the higher-mass systems tends to widen
    the distribution toward the high-mass end and, in models that
    allow it, produce a second, higher-mass peak in addition to the
    one in Figure \ref{fig:dists}. }
\end{figure}

Figures \ref{fig:power-law-high} presents the marginalized
distribution for the three power-law parameters $\Mmin$, $\Mmax$, and
$\alpha$ (Section \ref{sec:power-law}) from an analysis including the
higher-mass systems.  The distribution for $\Mmax$ is quite broad
because the best fit power laws slope downward ($\alpha < 0$), making
this parameter less relevant.  The range $-5.05 \leq \alpha \leq
-1.77$ encloses 90\% of the probability; the median value of $\alpha$
is -3.23.  The presence of the higher-mass samples in the analysis
produces a distinctive tail, eliminating the correlations discussed in
Section \ref{sec:power-law} and displayed in Figure
\ref{fig:power-law-2D} for the lower-mass subset of the observations.

\begin{figure}
  \begin{center}
    \plotone{plots/power-law-high}
  \end{center}
  \caption{\label{fig:power-law-high} A histogram of the marginalized
    distribution for the three parameters $\Mmin$, $\Mmax$, and
    $\alpha$ from the power-law model including the higher-mass
    samples in the MCMC.  The distribution for $\Mmax$ is quite broad
    because the best fit power laws slope downward ($\alpha < 0$),
    making this parameter less relevant.  The range $-5.05 \leq \alpha
    \leq -1.77$ encloses 90\% of the probability; the median value of
    $\alpha$ is -3.23.  The presence of the higher-mass samples in the
    analysis produces a distinctive tail, eliminating the correlations
    discussed in Section \ref{sec:power-law} and displayed in Figure
    \ref{fig:power-law-2D} for the lower-mass subset of the
    observations. }
\end{figure}

Figure \ref{fig:exp-cutoff-high} displays the marginalized
distributions for the exponential parameters $\Mmin$ and $M_0$
(Section \ref{sec:exponential}) from an analysis including the
higher-mass systems.  The distribution for the scale mass, $M_0$, has
moved to higher masses relative to Figure \ref{fig:exp-marginal} to
fit the tail of the mass distribution; the distribution for $\Mmin$ is
less affected, though it has broadened somewhat toward low masses.

\begin{figure}
  \begin{center}
    \plotone{plots/exp-cutoff-high}
  \end{center}
  \caption{\label{fig:exp-cutoff-high} The marginalized distributions
    for the exponential parameters $\Mmin$ and $M_0$ (Section
    \ref{sec:exponential}) from an analysis including the higher-mass
    systems.  The distribution for the scale mass, $M_0$, has moved to
    higher masses relative to Figure \ref{fig:exp-marginal} to fit the
    tail of the mass distribution; we now have $2.8292 \leq M_0 \leq
    7.9298$ with 90\% confidence, with median 4.7003.  The
    distribution for $\Mmin$ is less affected, though it has broadened
    somewhat toward low masses.}
\end{figure}

Figure \ref{fig:gaussian-high} displays the marginalized distributions
for the Gaussian parameters (Section \ref{sec:gaussian}) when the
higher-mass objects are included in the mass distribution.  The mean
mass, $\mu$, and the mass standard deviation, $\sigma$, are both
increased relative to Figure \ref{fig:gaussian} to account for the
broader distribution and higher-mass tail.

\begin{figure}
  \begin{center}
    \plotone{plots/gaussian-high}
  \end{center}
  \caption{\label{fig:gaussian-high} The marginalized distributions
    for the Gaussian parameters when the higher-mass objects are
    included in the mass distribution.  The mean mass, $\mu$, and the
    mass standard deviation, $\sigma$, are both increased relative to
    Figure \ref{fig:gaussian} to account for the broader distribution
    and higher-mass tail.  The peak of the underlying mass
    distribution lies in the range $7.8660 \leq \mu \leq 10.9836$ with
    90\% confidence; the median value is 9.2012.}
\end{figure}

The analysis of the two-Gaussian model shows the largest change when
the higher-mass samples are included.  Figure
\ref{fig:two-gaussian-high} shows the marginalized distributions for
the two-Gaussian parameters (Section \ref{sec:gaussian}) when the
higher-mass samples are included in the analysis.  In stark contrast
to Figure \ref{fig:two-gaussian}, there are two well-defined,
separated peaks; the lower-mass peak reproduces the results from the
low-mass samples, while the higher-mass peak ($13.5534 \leq \mu_2 \leq
27.9481$ with 90\% confidence; median 20.3839) matches the new
higher-mass samples.  The peak in $\alpha$ near 0.8 is consistent with
approximately 4/5 the total probability being concentrated in the 15
low-mass samples.

\begin{figure}
  \begin{center}
    \plotone{plots/two-gaussian-high}
  \end{center}
  \caption{\label{fig:two-gaussian-high} The marginalized
    distributions for the two-Gaussian parameters (Section
    \ref{sec:gaussian}) when the higher-mass samples are included in
    the analysis.  In stark contrast to Figure \ref{fig:two-gaussian},
    there are two well-defined, separated peaks; the lower-mass peak
    reproduces the results from the low-mass samples, while the
    higher-mass peak ($13.5534 \leq \mu_2 \leq 27.9481$ with 90\%
    confidence; median 20.3839) matches the new higher-mass samples.
    The peak in $\alpha$ near 0.8 is consistent with approximately 15
    out of 20 samples belonging to the low-mass peak.}
\end{figure}

The marginalized distributions for the log-normal parameters (Section
\ref{sec:log-normal}) when the higher-mass samples are included in the
analysis are displayed in Figure \ref{fig:log-normal-high}.  The
changes when the higher-mass samples are included (compare to Figure
\ref{fig:log-normal}) are similar to the changes in the Gaussian
distribution: the mean mass moves to higher masses, and the
distribution broadens.  Because the log-normal distribution is
inherently asymmetric, with a high-mass tail, it does not need to
widen as much as the Gaussian distribution did.

\begin{figure}
  \begin{center}
    \plotone{plots/log-normal-high}
  \end{center}
  \caption{\label{fig:log-normal-high} The marginalized distributions
    for the log-normal parameters (Section \ref{sec:log-normal}) when
    the higher-mass samples are included in the analysis.  The changes
    when the higher-mass samples are included (compare to Figure
    \ref{fig:log-normal}) are similar to the changes in the Gaussian
    distribution: the mean mass moves to higher masses, and the
    distribution broadens.}
\end{figure}

The confidence limits on the parameters for the parametric models of
the underlying mass distribution are displayed in Table
\ref{tab:high-mass-parametric} (compare to Table
\ref{tab:low-mass-parametric}).

\begin{table}
  \begin{center}
    \begin{tabular}{|l|c|c|c|c|c|c|}
      \hline
      Model & Parameter & 5\% & 15\% & 50\% & 85\% & 95\% \\
      \hline \hline
      Power Law (Equation \eqref{eq:power-law-dist}) & $\Mmin$ & 
      4.87141 & 5.29031 & 5.85019 & 6.26118 & 6.45674 \\
      \hline
      & $\Mmax$ & 19.1097 & 23.4242 & 31.5726 & 37.7519 & 39.3369 \\
      \hline
      & $\alpha$ & -5.04879 & -4.30368 & -3.23404 & -2.31365 & -1.77137 \\
      \hline \hline
      Decaying Exponential (Equation \eqref{eq:exp-def}) & $\Mmin$ & 
      4.0865 & 4.60236 & 5.32683 & 5.94097 & 6.22952 \\
      \hline
      & $M_0$ & 2.82924 & 3.41139 & 4.70034 & 6.52214 & 7.92979 \\
      \hline \hline
      Gaussian (Equation \eqref{eq:gaussian-def}) & $\mu$ & 
      7.86599 & 8.33118 & 9.20116 & 10.2493 & 10.9836 \\
      \hline
      & $\sigma$ & 2.23643 & 2.58899 & 3.33545 & 4.17886 & 4.67881 \\
      \hline \hline
      Two Gaussian (Equation \eqref{eq:two-gaussian-def}) & $\mu_1$ & 
      6.741 & 7.02724 & 7.48174 & 8.0139 & 8.46626 \\
      \hline
      & $\mu_2$ & 13.5534 & 16.202 & 20.3839 & 24.9259 & 27.9481 \\
      \hline
      & $\sigma_1$ & 0.742824 & 0.913941 & 1.31244 & 1.94862 & 2.50238 \\
      \hline
      & $\sigma_2$ & 0.511159 & 1.5025 & 4.39824 & 7.04612 & 8.25905 \\
      \hline
      & $\alpha$ & 0.575692 & 0.670978 & 0.798227 & 0.891522 & 0.932143 \\
      \hline \hline
      Log Normal (Equation \eqref{eq:log-normal-def}) & $\langle M \rangle$ & 
      8.00086 & 8.51192 & 9.6264 & 11.1851 & 12.3986 \\
      \hline
      & $\sigma_M$ & 2.19262 & 2.8137 & 4.16742 & 6.25101 & 8.11839 \\
      \hline
    \end{tabular}
  \end{center}
  \caption{\label{tab:high-mass-parametric} Quantiles of the
    marginalized distribution for each of the parameters in the models
    discussed in Section \ref{sec:parametric-models} when the
    higher-mass samples are included in the analysis (compare to Table
    \ref{tab:low-mass-parametric}).  We indicate the 5\%, 15\%, 50\% (median), 85\%, and 95\% quantiles.}
\end{table}

The non-parametric (histogram; see Section \ref{sec:non-parametric-models}) models
also show evidence of a long tail from the inclusion of the
higher-mass samples.  Table \ref{tab:high-mass-non-parametric}
displays confidence limits on the histogram parameters for the
analysis including the higher-mass systems; compare to Table
\ref{tab:low-mass-non-parametric}.

\begin{table}
  \begin{center}
    \begin{tabular}{|c|c|c|c|c|c|c|}
      \hline
      Bins & Boundary & 5\% & 15\% & 50\% & 85\% & 95\% \\
      \hline \hline
      \hline
      1 & $w_0$ & 2.22294 & 3.12695 & 4.2456 & 5.15132 & 5.58265 \\
      \hline
      & $w_1$ & 15.93 & 16.2535 & 17.7836 & 20.5449 & 22.5836 \\
      \hline \hline
      2 & $w_0$ & 3.87202 & 4.49983 & 5.41234 & 6.08334 & 6.35933 \\
      \hline
      & $w_1$ & 7.22163 & 8.25079 & 8.93669 & 9.71551 & 10.4287 \\
      \hline
      & $w_2$ & 18.4762 & 19.9798 & 24.941 & 32.5972 & 36.8615 \\
      \hline \hline
      3 & $w_0$ & 3.39289 & 4.24509 & 5.41694 & 6.15087 & 6.42822 \\
      \hline
      & $w_1$ & 6.41849 & 6.71984 & 7.47263 & 8.2942 & 8.61785 \\
      \hline
      & $w_2$ & 8.41449 & 8.64664 & 9.17056 & 10.4075 & 12.2718 \\
      \hline
      & $w_3$ & 18.5705 & 21.0481 & 27.1494 & 34.7753 & 38.0652 \\
      \hline \hline
      4 & $w_0$ & 2.42094 & 3.69875 & 5.2596 & 6.25449 & 6.54316 \\
      \hline
      & $w_1$ & 5.83725 & 6.2836 & 6.84987 & 7.8033 & 8.27706 \\
      \hline
      & $w_2$ & 6.94919 & 7.43628 & 8.38531 & 9.13401 & 9.91845 \\
      \hline
      & $w_3$ & 8.50371 & 8.75188 & 9.86694 & 17.1848 & 22.1086 \\
      \hline
      & $w_4$ & 18.5823 & 21.4628 & 28.367 & 35.8118 & 38.5278 \\      
      \hline \hline
      5 & $w_0$ & 1.73691 & 3.19184 & 4.89769 & 5.9547 & 6.35522 \\
      \hline
      & $w_1$ & 5.46124 & 5.95881 & 6.59431 & 7.26795 & 7.91821 \\
      \hline
      & $w_2$ & 6.63468 & 6.9804 & 7.93239 & 8.60918 & 9.06926 \\
      \hline
      & $w_3$ & 7.89654 & 8.35634 & 8.91766 & 10.6568 & 13.9644 \\
      \hline
      & $w_4$ & 8.74064 & 9.42672 & 15.8004 & 22.7101 & 27.6399 \\
      \hline
      & $w_5$ & 20.0202 & 22.9065 & 29.6307 & 36.6606 & 38.8573 \\
      \hline
    \end{tabular}
  \end{center}
  \caption{\label{tab:high-mass-non-parametric} The 5\%, 15\%, 50\%
    (median), 85\%, and 95\% quantiles for the bin boundaries in the
    one- through five-bin histogram models discussed in Section
    \ref{sec:non-parametric-models} in an 
    analysis including the higher-mass, wind-fed systems.  
    The tails evident in Figure \ref{fig:high-mass-dists} are apparent
    here as well; compare to Table \ref{tab:low-mass-non-parametric}.}
\end{table}

\subsection{Model Selection}

Repeating the model selection analysis discussed in Section
\ref{sec:model-selection} for the sample including the higher-mass
systems, we find that the model probabilities have changed with the
inclusion of the extra five systems.  This is conclusive evidence (as
if more were needed) that the higher-mass, wind-fed systems come from
a different underlying population than the lower-mass, Roche-lobe
overflow systems.  As before, we assume for this analysis that the
model priors are equal.

Reversible jump MCMC calculations of the model probabilities are
displayed in Figure \ref{fig:high-rj-evidence}; compare Figure
\ref{fig:rj}.  The exponential model is the most favored model for the
complete sample, with the two-Gaussian model the second-most favored.
The ranking of models differs significantly from the lower-mass
samples.  The model selection provides more strong evidence that the
wind-fed and Roche-lobe systems come from different underlying
populations.  The improvement of the exponential model relative to the
lower-mass analysis is encouraging for theoretical calculations that
attempt to model the entire population of X-ray binaries with this
mass model.  Note also that the increasing structure of the mass
distribution favors histogram models with three bins over those with
fewer bins.

\begin{figure}
  \begin{center}
    \plotone{plots/rj-high}
  \end{center}
  \caption{\label{fig:high-rj-evidence} The relative probability of
    the models discussed in Section \ref{sec:models} as computed using
    the reversible-jump MCMC with the efficient jump proposal
    algorithm described in Section \ref{sec:reversible-jump-mcmc},
    applied to all 20 systems in Table \ref{tab:sources} (i.e.\
    including the higher-mass systems).  In increasing order along the
    $x$-axis, the models are the power-law of Section
    \ref{sec:power-law} (PL), the decaying exponential of Section
    \ref{sec:exponential} (E), the single Gaussian of Section
    \ref{sec:gaussian} (G), the double Gaussian of Section
    \ref{sec:gaussian} (TG), and the one-, two-, three-, four-, and
    five-bin histogram models of Section
    \ref{sec:non-parametric-models} (H1, H2, H3, H4, H5,
    respectively).  The average of 500 independent reversible-jump
    MCMCs is plotted, along with the 1-$\sigma$ error on the average
    inferred from the standard deviation of the probability from the
    individual MCMCs.  Compare to Figure \ref{fig:rj}.}
\end{figure}


\section{The Minimum Mass of the Black Hole Mass Distribution}
\label{sec:minimum-mass}

It is interesting to use our models for the underlying mass
distribution of X-ray binary black holes to try to place constraints
on the minimum black hole mass.  \citet{Bailyn1998} addressed this
question in the context of a ``mass gap'' between the most massive
neutron stars and the least massive black holes.  The more recent
study of \citet{Ozel2010} also looked for a mass gap using a subset of
the models and systems presented here.  Both works found that the
minimum black hole mass is significantly above the maximum neutron
star mass.  

The distribution of the minimum black hole mass from the analysis of
the lower-mass samples is displayed in Figure \ref{fig:min-mass}.  For
all distributions, the minimum black hole mass is defined as the 1\%
mass quantile (i.e.\ the mass lying below 99\% of the mass
distribution).  (A cutoff quantile is necessary in the case of those
distributions that do not have a hard cutoff mass; even for those that
do, like the power-law model, it can be useful to define a ``soft''
cutoff in the event that the lower mass hard cutoff becomes an
irrelevant parameter as discussed in Section \ref{sec:power-law}.)

\begin{figure}
  \begin{center}
    \plottwo{plots/mmin-parametric}{plots/mmin-non-parametric}
  \end{center}
  \caption{\label{fig:min-mass} The distributions for the minimum
    black hole mass calculated from the MCMC samples for the models in
    Section \ref{sec:models}.  The minimum black hole mass is defined
    as the 1\% mass quantile.  For the most favored models, the
    power-law and Gaussian, the 90\% confidence limit on the minimum
    black hole mass is 4.3 $\Msun$ and 2.9 $\Msun$, respectively.}
\end{figure}

We find that the best-fit model (the power-law) has a 90\% confidence
limit on the minimum black hole mass of 4.3 $\Msun$.  This is
significantly above the maximum theoretically-allowed neutron star
mass, $\sim 3 \Msun$ \citep{Kalogera1996}\fixme{Vicky, do you have a
  more recent reference than this?}, so we conclude that the
lower-mass systems show strong evidence of a mass gap.

The distribution of minimum black hole masses for the analysis of the
complete sample (i.e.\ including the higher-mass systems) is shown in
Figure \ref{fig:high-min-mass}.  For the most favored model, the
exponential, the 90\% confidence limit on the minimum black hole mass
is 4.5 $\Msun$.  We therefore conclude that there is strong evidence
for a mass gap in the complete sample as well.

\begin{figure}
\begin{center}
    \plottwo{plots/mmin-parametric-high}{plots/mmin-non-parametric-high}
  \end{center}
  \caption{\label{fig:high-min-mass} The distributions for the minimum
    black hole mass calculated from the MCMC samples for the models in
    Section \ref{sec:models} using the complete sample of systems.
    The minimum black hole mass is defined as the 1\% mass quantile.
    For the most favored models, the power-law, exponential and
    two-Gaussian, the 90\% confidence limit on the minimum black hole
    mass is 5.2 $\Msun$, 4.5 $\Msun$, and 2.2 $\Msun$, respectively.}
\end{figure}

\section{Conclusion}

\acknowledgements

This work was supported by grant \fixme{XXX}.  IM acknowledges support
from the NSF AAPF under award AST-0901985.  Calculations for this work
were performed on the Northwestern Fugu cluster, which was partially
funded by NSF MRI grant \fixme{XXX}.  We thank Jonathan Gair for
helpful discussions.

\bibliography{paper}

\end{document}